我现在需要开发两个demo应用，分别设备端(DeviceApp)和手机端(ControlApp)。功能是做一个同屏控制功能，由手机控制端发起，获取设备端的屏幕，进行屏幕控制。设备端是基于Android12的操作系统上的安卓应用，手机端是使用flutter实现的运行在苹果和安卓应用。以下为整体的架构设计和功能列表，请按照内容，和功能需求进行详细的开发。

## 架构设计

### 整体架构
```
手机控制端(Flutter)
    ↕ WebSocket (信令)
    ↕ WebRTC (音视频 + 数据通道)
设备端(Android12系统应用)
    ↕ 硬件编码器(MediaCodec)
    ↕ 屏幕捕获(MediaProjection)
Android系统Framework
```

### 核心技术栈

**设备端(Android)**:
- **WebSocket服务器**: Java-WebSocket
- **WebRTC**: Google WebRTC原生库
- **屏幕捕获**: MediaProjection + VirtualDisplay
- **硬件编码**: MediaCodec (H.264硬编)
- **控制注入**: AccessibilityService

**手机端(Flutter)**:
- **WebSocket客户端**: web_socket_channel
- **WebRTC**: flutter_webrtc
- **视频渲染**: RTCVideoView
- **触控处理**: GestureDetector

## 详细设计方案

### 1. 通信架构
- **信令通道**: WebSocket处理连接建立、SDP交换、ICE候选
- **媒体通道**: WebRTC PeerConnection传输视频流
- **控制通道**: WebRTC DataChannel传输触控事件(低延迟)

### 2. 设备端核心模块

#### 屏幕捕获 + 硬件编码模块
```java
MediaProjection → VirtualDisplay → Surface → MediaCodec(硬件H.264编码) → WebRTC
```

**关键实现点**:
- 使用`MediaCodec.createEncoderByType("video/avc")`确保硬件编码
- 通过`Surface`直接连接VirtualDisplay和编码器，避免CPU拷贝
- 配置CBR编码模式，码率2-5Mbps，帧率30fps
- 启用B帧和低延迟模式

#### WebRTC集成
- 自定义VideoCapturer继承CameraCapturer
- 将MediaCodec编码输出直接送入WebRTC管道
- 配置WebRTC使用硬件编码器优先级

### 3. 延迟优化策略

1. **编码优化**:
   - GOP设置为30帧，I帧间隔1秒
   - 启用硬件编码器的低延迟模式
   - 动态调节码率和分辨率

2. **WebRTC优化**:
   - 禁用音频处理减少开销
   - 设置最小缓冲延迟
   - 启用实时传输优化

3. **网络优化**:
   - 本地网络环境下优化ICE策略
   - 使用TURN服务器提高连通性

## 完整任务清单

### 阶段1: 基础框架搭建

#### 任务1.1: 设备端WebSocket服务器
- [ ] 使用gradle搭建基础Android项目
- [ ] 集成Java-WebSocket库
- [ ] 实现WebSocket服务器启动和端口监听
- [ ] 添加客户端连接管理和心跳机制
- [ ] 实现设备信息广播(设备名、IP、端口)
- [ ] 添加连接认证和安全机制

#### 任务1.2: 手机端WebSocket客户端
- [ ] Flutter项目搭建和依赖配置
- [ ] 集成web_socket_channel插件
- [ ] 实现设备发现界面(扫描局域网设备)
- [ ] 建立WebSocket连接和重连逻辑
- [ ] 实现基础的信令消息收发

#### 任务1.3: WebRTC信令服务
- [ ] 设备端集成Google WebRTC库
- [ ] 手机端集成flutter_webrtc插件
- [ ] 实现SDP offer/answer交换
- [ ] 实现ICE候选交换
- [ ] 建立PeerConnection连接

### 阶段2: 屏幕捕获与硬件编码

#### 任务2.1: 屏幕捕获模块
- [ ] 申请MediaProjection权限(系统应用无需用户授权)
- [ ] 创建VirtualDisplay用于屏幕镜像
- [ ] 实现屏幕分辨率和方向检测
- [ ] 添加屏幕捕获开始/停止控制
- [ ] 处理屏幕旋转和分辨率变化

#### 任务2.2: 硬件编码器集成
- [ ] 初始化MediaCodec硬件H.264编码器
- [ ] 配置编码参数(码率、帧率、GOP)
- [ ] 创建Surface并连接到VirtualDisplay
- [ ] 实现编码输出数据回调处理
- [ ] 添加编码器异常处理和重启机制

#### 任务2.3: WebRTC视频源集成
- [ ] 自定义VideoCapturer继承CapturerObserver
- [ ] 将MediaCodec输出帧送入WebRTC管道
- [ ] 实现视频帧格式转换
- [ ] 配置WebRTC使用硬件编码优先级
- [ ] 测试端到端视频传输

### 阶段3: 控制功能实现

#### 任务3.1: 手机端触控捕获
- [ ] 实现视频显示界面(RTCVideoView)
- [ ] 添加GestureDetector捕获触控事件
- [ ] 实现坐标转换(手机屏幕→设备屏幕)
- [ ] 支持单点触控、多点触控和手势
- [ ] 通过WebRTC DataChannel发送控制指令

#### 任务3.2: 设备端控制注入
- [ ] 实现AccessibilityService权限申请
- [ ] 创建触控事件注入服务
- [ ] 解析DataChannel接收的控制指令
- [ ] 将触控坐标转换为MotionEvent
- [ ] 通过AccessibilityService注入触控事件

#### 任务3.3: 控制指令优化
- [ ] 实现触控事件的批量处理和去重
- [ ] 添加控制延迟监控
- [ ] 实现手势识别(点击、滑动、长按)
- [ ] 添加震动反馈和音效反馈

### 阶段4: 性能优化与稳定性

#### 任务4.1: 编码性能优化
- [ ] 实现动态码率调节(根据网络状况)
- [ ] 优化GOP结构和I帧间隔
- [ ] 添加帧率自适应调节
- [ ] 实现分辨率动态调整
- [ ] 优化内存使用和GC压力

#### 任务4.2: 网络传输优化
- [ ] 实现WebRTC传输质量监控
- [ ] 添加丢包检测和重传机制
- [ ] 优化ICE连接策略
- [ ] 实现网络切换处理
- [ ] 添加带宽测试和调节

#### 任务4.3: 延迟监控与优化
- [ ] 实现端到端延迟测量
- [ ] 添加各阶段延迟统计(捕获、编码、传输、显示)
- [ ] 实现延迟优化策略自动调节

### 阶段5: 用户界面与体验

#### 任务5.1: 手机端UI完善
- [ ] 设计和实现设备列表界面
- [ ] 创建连接状态显示
- [ ] 添加视频质量设置面板
- [ ] 实现全屏模式和窗口模式切换

#### 任务5.2: 交互体验优化
- [ ] 实现横竖屏自动适配
- [ ] 添加触控敏感度调节
- [ ] 实现虚拟按键(返回、主页、菜单)
- [ ] 实现截图和录屏功能

#### 任务5.3: 设备端管理界面
- [ ] 创建系统设置中的控制面板
- [ ] 显示当前连接状态和客户端信息
- [ ] 添加访问权限管理
- [ ] 实现服务自启动配置

### 阶段6: 测试与部署

#### 任务6.1: 功能测试
- [ ] 单元测试覆盖核心模块
- [ ] 集成测试验证端到端功能
- [ ] 性能测试(延迟、帧率、码率)
- [ ] 兼容性测试(不同设备和网络环境)
- [ ] 压力测试(长时间运行稳定性)

#### 任务6.2: 优化与修复
- [ ] 根据测试结果优化性能瓶颈
- [ ] 修复发现的bug和异常情况
- [ ] 完善错误处理和用户提示
- [ ] 优化资源占用和电池消耗
- [ ] 添加崩溃日志收集和分析

#### 任务6.3: 文档与部署
- [ ] 编写技术文档和API说明
- [ ] 创建用户使用指南
- [ ] 设置持续集成和部署流程
- [ ] 准备版本发布和更新机制
